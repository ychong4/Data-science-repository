{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8220453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Hierarchical Clustering\n",
    "#Complete Linkage, Average Linkage\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import ndimage \n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.spatial import distance_matrix \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import manifold, datasets \n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.datasets import make_blobs \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data\n",
    "# n_samples: The total number of points equally divided among clusters.\n",
    "# centers: The number of centers to generate, or the fixed center locations.\n",
    "# cluster_std: The standard deviation of the clusters. The larger the number, the further apart the clusters\n",
    "X1, y1 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb420ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering class require two inputs\n",
    "# n_clusters: The number of clusters to form as well as the number of centroids to generate.\n",
    "# linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. \n",
    "# The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "# 'complete', 'average'\n",
    "\n",
    "agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')\n",
    "agglom.fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29313331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure of size 6 inches by 4 inches.\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# These two lines of code are used to scale the data points down,\n",
    "# Or else the data points will be scattered very far apart.\n",
    "\n",
    "# Create a minimum and maximum range of X1.\n",
    "x_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)\n",
    "\n",
    "# Get the average distance for X1.\n",
    "X1 = (X1 - x_min) / (x_max - x_min)\n",
    "\n",
    "# This loop displays all of the datapoints.\n",
    "for i in range(X1.shape[0]):\n",
    "    # Replace the data points with their respective cluster value \n",
    "    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n",
    "    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),\n",
    "             color=plt.cm.nipy_spectral(agglom.labels_[i] / 10.),\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "    \n",
    "# Remove the x ticks, y ticks, x and y axis\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "#plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# Display the plot of the original data before clustering\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='.')\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendogram associated for the agglomerative hierarchical clustering\n",
    "# Distance matrix contains the distance from each point to every other point of a dataset\n",
    "# Distance matrix requires two inputs\n",
    "\n",
    "dist_matrix = distance_matrix(X1,X1) \n",
    "print(dist_matrix)\n",
    "\n",
    "# Using the linkage class from hierarchy, pass in the parameters: (1) the distance matrix, (2) 'complete' for complete linkage\n",
    "Z = hierarchy.linkage(dist_matrix, 'complete')\n",
    "\n",
    "# Using the dendogram class from hierarchy, pass in the parameter\n",
    "dendro = hierarchy.dendrogram(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLustering on Vehicle dataset\n",
    "\n",
    "filename = 'cars_clus.csv'\n",
    "\n",
    "#Read csv\n",
    "pdf = pd.read_csv(filename)\n",
    "print (\"Shape of dataset: \", pdf.shape)\n",
    "\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Drop the rows that have null value\n",
    "\n",
    "print (\"Shape of dataset before cleaning: \", pdf.size)\n",
    "pdf[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']] = pdf[['sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\n",
    "pdf = pdf.dropna()\n",
    "pdf = pdf.reset_index(drop=True)\n",
    "print (\"Shape of dataset after cleaning: \", pdf.size)\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c43c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# MinMaxScaler transforms features by scaling each feature to a given range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = featureset.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "feature_mtx = min_max_scaler.fit_transform(x)\n",
    "feature_mtx [0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc32ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using Scipy\n",
    "\n",
    "import scipy\n",
    "leng = feature_mtx.shape[0]\n",
    "D = scipy.zeros([leng,leng])\n",
    "for i in range(leng):\n",
    "    for j in range(leng):\n",
    "        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In agglomerative clustering, at each iteration, the algorithm must update the distance matrix to reflect the distance of the\n",
    "# newly formed cluster with the remaining clusters in the forest.\n",
    "# The following methods are supported in Scipy for calculating the distance between the newly formed cluster and each:\n",
    "# - single - complete - weighted - centroid\n",
    "\n",
    "import pylab\n",
    "import scipy.cluster.hierarchy\n",
    "Z = hierarchy.linkage(D, 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering does not require a pre-sepcified number of clusters.\n",
    "# However, some applications we want a partition of disjoint clusters just as in flat clustering.\n",
    "# So, you can use a cutting line\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 3\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "clusters\n",
    "\n",
    "# You can also determine the number of clusters directly\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "k = 5\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "clusters\n",
    "\n",
    "# Plot the dendrogram\n",
    "fig = pylab.figure(figsize=(18,50))\n",
    "def llf(id):\n",
    "    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n",
    "    \n",
    "dendro = hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a239fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLustering using scikit-learn\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist_matrix = euclidean_distances(feature_mtx,feature_mtx) \n",
    "print(dist_matrix)\n",
    "\n",
    "Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')\n",
    "\n",
    "fig = pylab.figure(figsize=(18,50))\n",
    "def llf(id):\n",
    "    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n",
    "    \n",
    "dendro = hierarchy.dendrogram(Z_using_dist_matrix,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgglomerativeClustering function from scikit-learn library to cluster the dataset\n",
    "# Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach \n",
    "# and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "# Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "# Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "\n",
    "agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\n",
    "agglom.fit(dist_matrix)\n",
    "\n",
    "agglom.labels_\n",
    "\n",
    "# Add a new field to our dataframe to show the cluster of each row\n",
    "pdf['cluster_'] = agglom.labels_\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "n_clusters = max(agglom.labels_)+1\n",
    "colors = cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "cluster_labels = list(range(0, n_clusters))\n",
    "\n",
    "# Create a figure of size 6 inches by 4 inches.\n",
    "plt.figure(figsize=(16,14))\n",
    "\n",
    "for color, label in zip(colors, cluster_labels):\n",
    "    subset = pdf[pdf.cluster_ == label]\n",
    "    for i in subset.index:\n",
    "            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) \n",
    "    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)\n",
    "#    plt.scatter(subset.horsepow, subset.mpg)\n",
    "plt.legend()\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('horsepow')\n",
    "plt.ylabel('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 types of vehicles in our dataset, 'truck' and 'car'.\n",
    "# So, we use them to distinguish the classes, and summarize the cluster.\n",
    "# First, we count the number of cases in each group\n",
    "\n",
    "pdf.groupby(['cluster_','type'])['cluster_'].count()\n",
    "\n",
    "# Now, we can look at the characteristics of each cluster\n",
    "\n",
    "agg_cars = pdf.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()\n",
    "agg_cars\n",
    "\n",
    "# Cars:\n",
    "# Cluster 1: with almost high mpg, and low in horsepower\n",
    "# Cluster 2: with good mpg and horsepower, but higher price than average\n",
    "# Cluster 3: with low mpg, high horsepower, highest price\n",
    "\n",
    "# Trucks:\n",
    "# Cluster 1: with almost highest mpg among trucks, and lowest in horsepower and price.\n",
    "# Cluster 2: with almost low mpg and medium horsepower, but higher price than average.\n",
    "# Cluster 3: with good mpg and horsepower, low price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874be97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.figure(figsize=(16,10))\n",
    "for color, label in zip(colors, cluster_labels):\n",
    "    subset = agg_cars.loc[(label,),]\n",
    "    for i in subset.index:\n",
    "        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n",
    "    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\n",
    "plt.legend()\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('horsepow')\n",
    "plt.ylabel('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dc900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
